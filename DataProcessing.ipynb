{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb0a9b1-603b-40d8-b025-6e577f95914a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # For progress tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2bf40b0-277b-464c-87bc-ed6c71fcca2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1457/1457 [01:48<00:00, 13.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1457 images successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the folder path\n",
    "folder_path = r\"C:\\Users\\lsrin\\Downloads\\ResearchProject\\dataset\\Images\"\n",
    "\n",
    "# Define image size\n",
    "IMG_SIZE = (224, 224)  # Resize all images to 224x224\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_and_preprocess_images(folder_path):\n",
    "    images = []\n",
    "    filenames = []\n",
    "    \n",
    "    for file in tqdm(os.listdir(folder_path)):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        \n",
    "        # Load image using OpenCV\n",
    "        image = cv2.imread(file_path)\n",
    "        if image is None:\n",
    "            print(f\"Error loading: {file}\")\n",
    "            continue\n",
    "        \n",
    "        # Resize image\n",
    "        image = cv2.resize(image, IMG_SIZE)\n",
    "        \n",
    "        # Normalize image (convert to 0-1 range)\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        \n",
    "        images.append(image)\n",
    "        filenames.append(file)\n",
    "    \n",
    "    return np.array(images), filenames\n",
    "\n",
    "# Run the function\n",
    "images, filenames = load_and_preprocess_images(folder_path)\n",
    "\n",
    "print(f\"Loaded {len(images)} images successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baa1545-b34c-4c2b-bf27-fde400106ad8",
   "metadata": {},
   "source": [
    "## Image Preprocessing Steps\n",
    "### step 1: Resizing\n",
    "Resize images to a uniform shape (e.g., 224x224 for CNNs like ResNet or 128x128 for a lightweight model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d519a2-1e9f-4383-9ca6-f401240ac0ce",
   "metadata": {},
   "source": [
    "## Noise Reduction & Smoothing\n",
    "Apply Gaussian Blur or Median Filtering to remove noise while keeping skin texture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed89609-2dd8-4065-9de3-27f571bf3897",
   "metadata": {},
   "source": [
    "## Color Space Conversion\n",
    "Convert the image to HSV or YCrCb to highlight red (pimple) and dark (spot) regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f06d97-ba98-4e61-bb58-f8285d662cec",
   "metadata": {},
   "source": [
    "## Contrast Enhancement (CLAHE)\n",
    "Improves visibility of dark spots and pimples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f5de7f-fd20-4bfd-9282-7c4e30fcd284",
   "metadata": {},
   "source": [
    "## Thresholding & Edge Detection\n",
    "Thresholding: Helps segment pimples (red regions) and dark spots.\n",
    "Canny Edge Detection: Finds clear boundaries of acne."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54bc3a4-f2c4-464c-b87c-2476e016bed9",
   "metadata": {},
   "source": [
    "## Data Augmentation (Optional)\n",
    "Use horizontal flipping, brightness adjustment, and rotation to increase dataset variety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b41edf5f-2090-44a5-9576-b7bd953e8354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1457/1457 [00:05<00:00, 255.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessed 1457 images!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def preprocess_image(image):\n",
    "    # Convert to HSV and LAB color spaces\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    # Apply CLAHE for contrast enhancement\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    l = clahe.apply(l)\n",
    "    lab = cv2.merge((l, a, b))\n",
    "    image_clahe = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    # Apply noise removal\n",
    "    image_denoised = cv2.medianBlur(image_clahe, 5)\n",
    "\n",
    "    # Convert to grayscale and detect edges (for feature extraction)\n",
    "    gray = cv2.cvtColor(image_denoised, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 30, 100)\n",
    "\n",
    "    return image_denoised, edges\n",
    "\n",
    "# Apply preprocessing to all images\n",
    "preprocessed_images = []\n",
    "edge_maps = []\n",
    "\n",
    "for img in tqdm(images):\n",
    "    img_denoised, img_edges = preprocess_image((img * 255).astype(np.uint8))  # Convert back to 0-255 for OpenCV\n",
    "    preprocessed_images.append(img_denoised)\n",
    "    edge_maps.append(img_edges)\n",
    "\n",
    "print(f\" Preprocessed {len(preprocessed_images)} images!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1f9dfd-8b8b-41c2-bef0-0ce32547fca1",
   "metadata": {},
   "source": [
    "## step 2: save the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "448273a9-9018-4fbb-810a-c7116063d7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved preprocessed images in C:\\Users\\lsrin\\Downloads\\ResearchProject\\dataset\\Preprocessed\n"
     ]
    }
   ],
   "source": [
    "output_folder = r\"C:\\Users\\lsrin\\Downloads\\ResearchProject\\dataset\\Preprocessed\"\n",
    "\n",
    "# Create output folder if not exists\n",
    "#os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for i, img in enumerate(preprocessed_images):\n",
    "    save_path = os.path.join(output_folder, filenames[i])\n",
    "    cv2.imwrite(save_path, img)\n",
    "\n",
    "print(f\" Saved preprocessed images in {output_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ee09d1fd-0244-4fce-87b8-f40fe6e213e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6bedd5-a73e-4c4a-8b79-e898104ca54d",
   "metadata": {},
   "source": [
    "## USING PRETRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "86af3db4-3984-4181-9571-2edb8fbffa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, MobileNetV2, InceptionV3, EfficientNetB0\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "76631eac-37bb-430d-b2d1-d46b28c0fceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 5\n",
    "EPOCHS = 10\n",
    "\n",
    "# Define pretrained models\n",
    "pretrained_models = {\n",
    "    \"VGG16\": VGG16,\n",
    "    \"ResNet50\": ResNet50,\n",
    "    \"MobileNetV2\": MobileNetV2,\n",
    "    \"InceptionV3\": InceptionV3,\n",
    "    \"EfficientNetB0\": EfficientNetB0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8ddafdaa-ac0c-4d7b-be16-57b0feeece33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_images(folder_path):\n",
    "    images, labels = [], []\n",
    "    for file in tqdm(os.listdir(folder_path)):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        image = cv2.imread(file_path)\n",
    "        if image is None:\n",
    "            continue\n",
    "        image = cv2.resize(image, IMG_SIZE)\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        images.append(image)\n",
    "        \n",
    "        # Extract numeric severity   level from filenames like \"level1_6.jpg\"\n",
    "        label = ''.join([c for c in file if c.isdigit()])  # Extract digits from filename\n",
    "        if label:\n",
    "            labels.append(int(label[0]))  # Use the first digit as severity level\n",
    "        else:\n",
    "            continue  # Skip files that don't contain a valid severity level\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Extract features using a pretrained model\n",
    "def extract_features(model_name, images):\n",
    "    base_model = pretrained_models[model_name](weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    model = Model(inputs=base_model.input, outputs=Flatten()(base_model.output))\n",
    "    features = model.predict(images)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fdd05a04-20b0-436b-89b9-67aa3847f27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1457/1457 [00:02<00:00, 550.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "preprocessed_folder = r\"C:\\Users\\lsrin\\Downloads\\ResearchProject\\dataset\\Preprocessed\"\n",
    "X, y = load_preprocessed_images(preprocessed_folder)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "y = to_categorical(y, NUM_CLASSES)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fc0098-6bc4-478a-bb9f-049ef99db88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier using VGG16 features...\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1351s\u001b[0m 37s/step\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 10s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lsrin\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 140ms/step - accuracy: 0.4125 - loss: 2.0073 - val_accuracy: 0.4349 - val_loss: 1.4459\n",
      "Epoch 2/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 118ms/step - accuracy: 0.5687 - loss: 0.9897 - val_accuracy: 0.4897 - val_loss: 1.6279\n",
      "Epoch 3/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 124ms/step - accuracy: 0.7923 - loss: 0.5422 - val_accuracy: 0.5411 - val_loss: 0.9979\n",
      "Epoch 4/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 121ms/step - accuracy: 0.8365 - loss: 0.4117 - val_accuracy: 0.6164 - val_loss: 0.9073\n",
      "Epoch 5/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 117ms/step - accuracy: 0.8392 - loss: 0.3650 - val_accuracy: 0.6233 - val_loss: 0.9497\n",
      "Epoch 6/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 120ms/step - accuracy: 0.9228 - loss: 0.2300 - val_accuracy: 0.6644 - val_loss: 0.8996\n",
      "Epoch 7/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 119ms/step - accuracy: 0.9728 - loss: 0.1342 - val_accuracy: 0.6575 - val_loss: 0.8865\n",
      "Epoch 8/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 122ms/step - accuracy: 0.9742 - loss: 0.1110 - val_accuracy: 0.6370 - val_loss: 1.1114\n",
      "Epoch 9/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 119ms/step - accuracy: 0.9850 - loss: 0.0854 - val_accuracy: 0.6336 - val_loss: 1.1677\n",
      "Epoch 10/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 126ms/step - accuracy: 0.9720 - loss: 0.1075 - val_accuracy: 0.6507 - val_loss: 1.0904\n",
      "Training completed for VGG16\n",
      "Training classifier using ResNet50 features...\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 4s/step\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 4s/step\n",
      "Epoch 1/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 427ms/step - accuracy: 0.3495 - loss: 4.5234 - val_accuracy: 0.3973 - val_loss: 1.6463\n",
      "Epoch 2/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 408ms/step - accuracy: 0.3890 - loss: 1.5636 - val_accuracy: 0.4486 - val_loss: 1.2994\n",
      "Epoch 3/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 415ms/step - accuracy: 0.4237 - loss: 1.3382 - val_accuracy: 0.4418 - val_loss: 1.1358\n",
      "Epoch 4/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 414ms/step - accuracy: 0.4386 - loss: 1.2201 - val_accuracy: 0.4315 - val_loss: 1.5013\n",
      "Epoch 5/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 415ms/step - accuracy: 0.3943 - loss: 1.4355 - val_accuracy: 0.3801 - val_loss: 1.1796\n",
      "Epoch 6/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 419ms/step - accuracy: 0.4175 - loss: 1.1615 - val_accuracy: 0.4555 - val_loss: 1.2699\n",
      "Epoch 7/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 415ms/step - accuracy: 0.4281 - loss: 1.2009 - val_accuracy: 0.0959 - val_loss: 2.5272\n",
      "Epoch 8/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 412ms/step - accuracy: 0.3880 - loss: 1.4531 - val_accuracy: 0.4007 - val_loss: 1.2317\n",
      "Epoch 9/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 411ms/step - accuracy: 0.4947 - loss: 1.0782 - val_accuracy: 0.4897 - val_loss: 1.1089\n",
      "Epoch 10/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 416ms/step - accuracy: 0.4715 - loss: 1.0847 - val_accuracy: 0.4623 - val_loss: 1.1527\n",
      "Training completed for ResNet50\n",
      "Training classifier using MobileNetV2 features...\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1s/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1s/step \n",
      "Epoch 1/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 293ms/step - accuracy: 0.3965 - loss: 13.6015 - val_accuracy: 0.5616 - val_loss: 2.3592\n",
      "Epoch 2/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 277ms/step - accuracy: 0.7232 - loss: 1.2019 - val_accuracy: 0.5959 - val_loss: 2.2947\n",
      "Epoch 3/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 275ms/step - accuracy: 0.8923 - loss: 0.2927 - val_accuracy: 0.5890 - val_loss: 1.9466\n",
      "Epoch 4/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 287ms/step - accuracy: 0.9458 - loss: 0.1717 - val_accuracy: 0.5788 - val_loss: 2.2106\n",
      "Epoch 5/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 279ms/step - accuracy: 0.9669 - loss: 0.0997 - val_accuracy: 0.6233 - val_loss: 1.8494\n",
      "Epoch 6/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 279ms/step - accuracy: 0.9808 - loss: 0.0851 - val_accuracy: 0.6062 - val_loss: 1.9676\n",
      "Epoch 7/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 285ms/step - accuracy: 0.9854 - loss: 0.1065 - val_accuracy: 0.6096 - val_loss: 2.1375\n",
      "Epoch 8/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 281ms/step - accuracy: 0.9960 - loss: 0.0311 - val_accuracy: 0.6096 - val_loss: 2.0762\n",
      "Epoch 9/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 277ms/step - accuracy: 0.9957 - loss: 0.0533 - val_accuracy: 0.5788 - val_loss: 3.1978\n",
      "Epoch 10/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 281ms/step - accuracy: 0.9777 - loss: 0.0982 - val_accuracy: 0.5856 - val_loss: 2.3602\n",
      "Training completed for MobileNetV2\n",
      "Training classifier using InceptionV3 features...\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 3s/step\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 3s/step\n",
      "Epoch 1/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 258ms/step - accuracy: 0.3907 - loss: 10.1969 - val_accuracy: 0.5274 - val_loss: 2.4292\n",
      "Epoch 2/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 254ms/step - accuracy: 0.6708 - loss: 1.2667 - val_accuracy: 0.5411 - val_loss: 2.2864\n",
      "Epoch 3/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 227ms/step - accuracy: 0.7502 - loss: 0.7026 - val_accuracy: 0.4863 - val_loss: 1.8140\n",
      "Epoch 4/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 231ms/step - accuracy: 0.8609 - loss: 0.3238 - val_accuracy: 0.5651 - val_loss: 1.9900\n",
      "Epoch 5/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 225ms/step - accuracy: 0.9384 - loss: 0.1961 - val_accuracy: 0.5342 - val_loss: 1.5807\n",
      "Epoch 6/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 227ms/step - accuracy: 0.9665 - loss: 0.1073 - val_accuracy: 0.5993 - val_loss: 1.7583\n",
      "Epoch 7/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 222ms/step - accuracy: 0.9806 - loss: 0.0884 - val_accuracy: 0.5959 - val_loss: 1.8820\n",
      "Epoch 8/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 227ms/step - accuracy: 0.9747 - loss: 0.0734 - val_accuracy: 0.5925 - val_loss: 1.8205\n",
      "Epoch 9/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 221ms/step - accuracy: 0.9746 - loss: 0.0845 - val_accuracy: 0.5753 - val_loss: 1.6769\n",
      "Epoch 10/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 227ms/step - accuracy: 0.9675 - loss: 0.1044 - val_accuracy: 0.5445 - val_loss: 1.7604\n",
      "Training completed for InceptionV3\n",
      "Training classifier using EfficientNetB0 features...\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 2s/step\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2s/step\n",
      "Epoch 1/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 265ms/step - accuracy: 0.3432 - loss: 5.3602 - val_accuracy: 0.4315 - val_loss: 2.3318\n",
      "Epoch 2/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 257ms/step - accuracy: 0.3743 - loss: 2.2436 - val_accuracy: 0.3390 - val_loss: 1.4994\n",
      "Epoch 3/10\n",
      "\u001b[1m10/37\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 253ms/step - accuracy: 0.3728 - loss: 1.6012"
     ]
    }
   ],
   "source": [
    "# Train classifiers and visualize results\n",
    "history_dict = {}\n",
    "for model_name in pretrained_models.keys():\n",
    "    print(f'Training classifier using {model_name} features...')\n",
    "    X_train_features = extract_features(model_name, X_train)\n",
    "    X_val_features = extract_features(model_name, X_val)\n",
    "    \n",
    "    # Build classifier\n",
    "    classifier = Sequential([\n",
    "        Dense(256, activation='relu', input_shape=(X_train_features.shape[1],)),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    classifier.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train model\n",
    "    history = classifier.fit(X_train_features, y_train, validation_data=(X_val_features, y_val), epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "    history_dict[model_name] = history.history\n",
    "    \n",
    "    print(f'Training completed for {model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dd3e0a-4eaa-4a92-b788-a10ba7b36570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training results\n",
    "def plot_training_results(history_dict):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for model_name, history in history_dict.items():\n",
    "        plt.plot(history['accuracy'], label=f'{model_name} Accuracy')\n",
    "        plt.plot(history['val_accuracy'], label=f'{model_name} Val Accuracy', linestyle='dashed')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Model Accuracy Comparison')\n",
    "    plt.show()\n",
    "\n",
    "plot_training_results(history_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e2384f-d08f-4544-8e22-c2c5eadc6798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict severity level for a new image\n",
    "def predict_severity(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, IMG_SIZE)\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    \n",
    "    predictions = {}\n",
    "    for model_name in pretrained_models.keys():\n",
    "        features = extract_features(model_name, image)\n",
    "        classifier = Sequential([\n",
    "            Dense(256, activation='relu', input_shape=(features.shape[1],)),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(NUM_CLASSES, activation='softmax')\n",
    "        ])\n",
    "        classifier.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        prediction = classifier.predict(features)\n",
    "        predicted_class = np.argmax(prediction)\n",
    "        severity_level = label_encoder.inverse_transform([predicted_class])[0]\n",
    "        predictions[model_name] = severity_level\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Example usage\n",
    "image_path = r\"C:\\Users\\lsrin\\Downloads\\ResearchProject\\dataset\\Test\\sample.jpg\"\n",
    "print(predict_severity(image_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
